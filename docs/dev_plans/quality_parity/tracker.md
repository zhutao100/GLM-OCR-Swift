# Quality / parity validation

**Objective:** validate end-to-end OCR output quality and implementation parity for **GLM-OCR-Swift** across two complementary baselines:

1) **Parity baseline (upstream):** match the official project’s published example outputs (`examples/reference_result/*`) as closely as practical.
2) **Quality baseline (curated):** converge toward **human-verified** outputs in `examples/golden_result/*` when upstream examples are noisy, inconsistent, or known-bad.

This keeps “are we still functionally equivalent?” separate from “is the output actually good?” while preserving an opt-in workflow so `swift test` stays fast by default.

**Status (2026-02-17):** active — baselines are pinned; a report-only parity/quality harness exists (`scripts/compare_examples.py`); next step is to promote stable checks (thresholds) and fill remaining golden gaps (e.g. `GLM-4.5V_Page_1`).

---

## Artifact map (what each folder is for)

- `examples/source/`
  Input fixtures mirrored from the upstream GLM-OCR repo (`*.pdf`, `*.png`).

- `examples/reference_result/<name>/`
  Upstream “reference” outputs (Markdown + JSON + cropped images). Use this to measure **functional parity** and detect formatting/schema regressions.

- `examples/reference_result_notes/<name>/`
  Human notes about known issues/quirks in the upstream reference outputs (OCR mistakes, formatting oddities, schema inconsistencies).

- `examples/golden_result/<name>/`
  Human-verified “best known” outputs. Use this as the **quality calibration target**, even when it intentionally diverges from upstream reference outputs.

- `examples/result/<name>/`
  Outputs generated by this repo (via `scripts/run_examples.sh` or opt-in tests). Treat as ephemeral build artifacts.

---

## Baselines (pinned snapshots)

Comparisons only mean something if the model snapshots are pinned.

**Baseline revisions (2026-02-17):**

- GLM-OCR: `zai-org/GLM-OCR` @ `677c6baa60442a451f8a8c7eabdfab32d9801a0b`
- PP-DocLayout-V3: `PaddlePaddle/PP-DocLayoutV3_safetensors` @ `a0abee1e2bb505e5662993235af873a5d89851e3`

Where these come from:

- Hugging Face hub cache snapshots live under `~/.cache/huggingface/hub/models--*/snapshots/<commit-hash>`.
- The pinned hashes above are the snapshot folder names used for parity/quality reports and opt-in integration checks.
- These are also the current defaults in:
  - `Sources/ModelAdapters/GLMOCR/GLMOCRDefaults.swift`
  - `Sources/ModelAdapters/DocLayout/PPDocLayoutV3Defaults.swift`

Override policy:

- CLI: pass `--revision` and `--layout-revision` explicitly to use `main` (or another tag/commit).
- Batch runner: `scripts/run_examples.sh --glm-revision … --layout-revision …` forwards those to the CLI.

To intentionally update a baseline:

1) Regenerate `examples/result/*` with the new revision(s).
2) Re-run the report harness (below) and record the new commit hashes + rationale.
3) If updating `golden_result`, follow the update policy in `implementation_plan.md` (manual + reviewable).

---

## What “done” looks like

- A pinned snapshot/revision (GLM-OCR + PP-DocLayout-V3) recorded as the parity baseline for comparisons.
- Reproducible workflows for:
  - local, fast “does it still work?” checks (no downloads),
  - opt-in **parity** checks (end-to-end OCR vs `examples/reference_result/*`),
  - opt-in **quality** checks (end-to-end OCR vs `examples/golden_result/*`),
  - opt-in numerical golden checks when touching preprocessing/model math (`docs/golden_checks.md`).
- A small curated set of examples with documented expectations (match vs intentional diffs) and known gaps.

---

## Current state (what exists today)

- Opt-in numerical golden checks for GLM-OCR forward-pass slices and PP-DocLayout-V3 (see `docs/golden_checks.md`).
- Opt-in end-to-end layout examples **parity** tests for PDFs:
  - `examples/source/GLM-4.5V_Page_1.pdf`
  - `examples/source/GLM-4.5V_Pages_1_2_3.pdf`
  (see `Tests/GLMOCRAdapterTests/LayoutExamplesParityIntegrationTests.swift`).
- A batch runner that regenerates `examples/result/*` from `examples/source/*` via the CLI:
  - `scripts/run_examples.sh` (always runs in layout mode so it can emit JSON).
- A report-only parity/quality diff harness:
  - `scripts/compare_examples.py` (writes per-example diffs + combined status table under `.build/quality_parity/`).

What’s missing today:

- Threshold-gated examples (both lanes) once a subset is stable enough to enforce.
- `examples/golden_result/GLM-4.5V_Page_1/` so the quality lane covers both PDFs.

---
## Examples coverage matrix (source vs reference vs golden)

| Example | Source | Reference MD | Reference JSON | Golden MD | Notes | Parity tests | Golden quality checks |
|---|---|---:|---:|---:|---:|---:|---:|
| `GLM-4.5V_Page_1` | `GLM-4.5V_Page_1.pdf` | ✅ | ✅ | — | — | ✅ (PDF) | — |
| `GLM-4.5V_Pages_1_2_3` | `GLM-4.5V_Pages_1_2_3.pdf` | ✅ | ✅ | ✅ | ✅ | ✅ (PDF) | — |
| `code` | `code.png` | ✅ | ✅ | ✅ | ✅ | — | — |
| `handwritten` | `handwritten.png` | ✅ | ✅ | ✅ | ✅ | — | — |
| `page` | `page.png` | ✅ | ✅ | ✅ | — | — | — |
| `paper` | `paper.png` | ✅ | ✅ | ✅ | — | — | — |
| `seal` | `seal.png` | ✅ | ✅ | ✅ | — | — | — |
| `table` | `table.png` | ✅ | ✅ | ✅ | — | — | — |

**Immediate gaps to close:**

- Add `examples/golden_result/GLM-4.5V_Page_1/` (Markdown + `imgs/`) so the quality lane covers both PDFs.
- Add missing `examples/reference_result_notes/*` entries where upstream outputs have known issues.

---

## Status snapshot (generated)

This is a quick snapshot of the **currently committed** `examples/result/*` compared to both baselines.

Generated via:

```bash
PYENV_VERSION=venv313 pyenv exec python3 scripts/compare_examples.py --lane both
```

| Example | Parity MD | Parity JSON | Parity Images | Quality MD | Quality Images |
|---|---:|---:|---:|---:|---:|
| `GLM-4.5V_Page_1` | diff | diff | match | missing | match |
| `GLM-4.5V_Pages_1_2_3` | diff | diff | match | diff | match |
| `code` | diff | diff | match | diff | match |
| `handwritten` | match | match | match | diff | match |
| `page` | diff | diff | match | diff | match |
| `paper` | diff | diff | match | diff | match |
| `seal` | match | match | match | diff | match |
| `table` | match | diff | match | diff | match |

---

## Existing workflows (runnable today)

### 1) Regenerate `examples/result/*` from the CLI

```bash
scripts/run_examples.sh --clean
```

Then compare:

- parity: `examples/result/*` vs `examples/reference_result/*`
- quality: `examples/result/*` vs `examples/golden_result/*`

Consult `examples/reference_result_notes/*` for known upstream issues.

### 2) Generate parity/quality reports (report-only harness)

This compares the committed `examples/result/*` against:

- parity lane: `examples/reference_result/*`
- quality lane: `examples/golden_result/*`

```bash
PYENV_VERSION=venv313 pyenv exec python3 scripts/compare_examples.py --lane both
```

By default this writes per-example diffs and a machine-readable summary under:

- `.build/quality_parity/`

Start here for the combined per-example status table:

- `.build/quality_parity/summary.md`

Harness policies (current `scripts/compare_examples.py` behavior):

- Markdown: normalize line endings + trim trailing whitespace + strip leading/trailing blank lines, then compare for exact match.
- JSON (parity lane): compare pages/blocks/index/label, bbox drift within `--bbox-tolerance` (default: 15px), and normalized `content`.
- Images: default is `--image-policy exists` (file presence + extra/missing reporting); use `sha256` only if image encoding is stable.
- Gating (optional): add `--fail-on missing|markdown|json|images` to exit non-zero.

### 3) Run opt-in end-to-end examples parity tests (PDFs)

```bash
# For `swift test` (debug), ensure mlx.metallib exists for debug build products.
scripts/build_mlx_metallib.sh -c debug

GLMOCR_RUN_EXAMPLES=1 \
GLMOCR_SNAPSHOT_PATH=<local_glm_ocr_snapshot> \
LAYOUT_SNAPSHOT_PATH=<local_ppdoclayoutv3_snapshot> \
swift test --filter LayoutExamplesParityIntegrationTests
```

---

## How to use `examples/golden_result` (the intended model)

Golden results are **not** “expected output for today” by default; they are the **target** that guides quality improvements.

Use them to:

- **Calibrate postprocessing/formatting** (Markdown structure, headings, code-fence language tags, whitespace, punctuation).
- **Prioritize fixes**: if upstream reference outputs contain obvious OCR errors, treat golden as the ground truth for what “good” should look like.
- **Prevent regressions**: once an example’s output matches (or nearly matches) golden, lock that in with opt-in quality checks.

Golden results should be updated only via deliberate, human-reviewed changes (never by automated regeneration).

---

## Next tasks (prioritized)

### 1) Pin and record the baseline snapshots

Done (2026-02-17):

- Baselines recorded in this tracker (see “Baselines (pinned snapshots)”).
- Defaults pinned in code (`GLMOCRDefaults.revision`, `PPDocLayoutV3Defaults.revision`).
- CLI overrides supported via `--revision` and `--layout-revision`.

### 2) Introduce a **golden quality** evaluation lane

Done (report-only, 2026-02-17):

- Script-driven harness: `scripts/compare_examples.py --lane quality` (writes diffs + summary).
- Combined per-example status table: `scripts/compare_examples.py --lane both` → `.build/quality_parity/summary.md`.

Next:

- Add per-example threshold policies once at least 1–2 examples are stable enough to gate.

Implementation details and phasing live in:
- `docs/dev_plans/quality_parity/implementation_plan.md`

### 3) Expand end-to-end parity coverage (upstream lane)

Done (report-only, 2026-02-17):

- Parity reports now cover all `examples/source/*` via `scripts/compare_examples.py --lane parity`.

Next:

- Add opt-in `swift test` parity coverage for representative `*.png` examples once the parity target is tighter.

### 4) Make the “match” definition explicit per artifact

Done (2026-02-17): policies are documented in this tracker and implemented in the harness.

- Markdown: exact match after normalization.
- JSON: page/block/index/label + bbox tolerance + normalized content.
- Images: file presence (optional sha256).

### 5) Operationalize the workflow

Done (2026-02-17): docs include runnable commands for generating results and producing reports.

---

## Exit criteria

- A documented baseline snapshot + a repeatable parity workflow (**reference_result** lane).
- A documented baseline snapshot + a repeatable quality workflow (**golden_result** lane).
- The opt-in examples suite covers both PDFs and representative image cases.
- Remaining gaps are explicit and tracked per example (no “unknown parity/quality” phrasing once the table is in place).
