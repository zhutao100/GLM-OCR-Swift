# Quality / parity validation

**Objective:** validate end-to-end OCR output quality and implementation parity for **GLM-OCR-Swift** across two complementary baselines:

1) **Parity baseline (upstream):** match the official project’s published example outputs (`examples/reference_result/*`) as closely as practical.
2) **Quality baseline (curated):** converge toward **human-verified** outputs in `examples/golden_result/*` when upstream examples are noisy, inconsistent, or known-bad.

This keeps “are we still functionally equivalent?” separate from “is the output actually good?” while preserving an opt-in workflow so `swift test` stays fast by default.

**Status (2026-02-17):** active — opt-in parity harness exists for a small PDF set; next step is to operationalize **golden_result** as a first-class quality target.

---

## Artifact map (what each folder is for)

- `examples/source/`
  Input fixtures mirrored from the upstream GLM-OCR repo (`*.pdf`, `*.png`).

- `examples/reference_result/<name>/`
  Upstream “reference” outputs (Markdown + JSON + cropped images). Use this to measure **functional parity** and detect formatting/schema regressions.

- `examples/reference_result_notes/<name>/`
  Human notes about known issues/quirks in the upstream reference outputs (OCR mistakes, formatting oddities, schema inconsistencies).

- `examples/golden_result/<name>/`
  Human-verified “best known” outputs. Use this as the **quality calibration target**, even when it intentionally diverges from upstream reference outputs.

- `examples/result/<name>/`
  Outputs generated by this repo (via `scripts/run_examples.sh` or opt-in tests). Treat as ephemeral build artifacts.

---

## What “done” looks like

- A pinned snapshot/revision (GLM-OCR + PP-DocLayout-V3) recorded as the parity baseline for comparisons.
- Reproducible workflows for:
  - local, fast “does it still work?” checks (no downloads),
  - opt-in **parity** checks (end-to-end OCR vs `examples/reference_result/*`),
  - opt-in **quality** checks (end-to-end OCR vs `examples/golden_result/*`),
  - opt-in numerical golden checks when touching preprocessing/model math (`docs/golden_checks.md`).
- A small curated set of examples with documented expectations (match vs intentional diffs) and known gaps.

---

## Current state (what exists today)

- Opt-in numerical golden checks for GLM-OCR forward-pass slices and PP-DocLayout-V3 (see `docs/golden_checks.md`).
- Opt-in end-to-end layout examples **parity** tests for PDFs:
  - `examples/source/GLM-4.5V_Page_1.pdf`
  - `examples/source/GLM-4.5V_Pages_1_2_3.pdf`
  (see `Tests/GLMOCRAdapterTests/LayoutExamplesParityIntegrationTests.swift`).
- A batch runner that regenerates `examples/result/*` from `examples/source/*` via the CLI:
  - `scripts/run_examples.sh` (always runs in layout mode so it can emit JSON).

What’s missing today:

- A **golden_result**-based quality harness (reporting + thresholds + test integration).
- A per-example status table that makes parity gaps vs quality gaps explicit.

---
## Examples coverage matrix (source vs reference vs golden)

| Example | Source | Reference MD | Reference JSON | Golden MD | Notes | Parity tests | Golden quality checks |
|---|---|---:|---:|---:|---:|---:|---:|
| `GLM-4.5V_Page_1` | `GLM-4.5V_Page_1.pdf` | ✅ | ✅ | — | — | ✅ (PDF) | — |
| `GLM-4.5V_Pages_1_2_3` | `GLM-4.5V_Pages_1_2_3.pdf` | ✅ | ✅ | ✅ | ✅ | ✅ (PDF) | — |
| `code` | `code.png` | ✅ | ✅ | ✅ | ✅ | — | — |
| `handwritten` | `handwritten.png` | ✅ | ✅ | ✅ | ✅ | — | — |
| `page` | `page.png` | ✅ | ✅ | ✅ | — | — | — |
| `paper` | `paper.png` | ✅ | ✅ | ✅ | — | — | — |
| `seal` | `seal.png` | ✅ | ✅ | ✅ | — | — | — |
| `table` | `table.png` | ✅ | ✅ | ✅ | — | — | — |

**Immediate gaps to close:**

- Add `examples/golden_result/GLM-4.5V_Page_1/` (Markdown + `imgs/`) so the quality lane covers both PDFs.
- Add missing `examples/reference_result_notes/*` entries where upstream outputs have known issues.

---

## Existing workflows (runnable today)

### 1) Regenerate `examples/result/*` from the CLI

```bash
scripts/run_examples.sh --clean
```

Then compare:

- parity: `examples/result/*` vs `examples/reference_result/*`
- quality: `examples/result/*` vs `examples/golden_result/*`

Consult `examples/reference_result_notes/*` for known upstream issues.

### 2) Run opt-in end-to-end examples parity tests (PDFs)

```bash
# For `swift test` (debug), ensure mlx.metallib exists for debug build products.
scripts/build_mlx_metallib.sh -c debug

GLMOCR_RUN_EXAMPLES=1 \
GLMOCR_SNAPSHOT_PATH=<local_glm_ocr_snapshot> \
LAYOUT_SNAPSHOT_PATH=<local_ppdoclayoutv3_snapshot> \
swift test --filter LayoutExamplesParityIntegrationTests
```

---

## How to use `examples/golden_result` (the intended model)

Golden results are **not** “expected output for today” by default; they are the **target** that guides quality improvements.

Use them to:

- **Calibrate postprocessing/formatting** (Markdown structure, headings, code-fence language tags, whitespace, punctuation).
- **Prioritize fixes**: if upstream reference outputs contain obvious OCR errors, treat golden as the ground truth for what “good” should look like.
- **Prevent regressions**: once an example’s output matches (or nearly matches) golden, lock that in with opt-in quality checks.

Golden results should be updated only via deliberate, human-reviewed changes (never by automated regeneration).

---

## Next tasks (prioritized)

### 1) Pin and record the baseline snapshots

- Decide which snapshot hashes (or commits) of:
  - `zai-org/GLM-OCR`
  - `PaddlePaddle/PP-DocLayoutV3_safetensors`
  are treated as the parity baseline.
- Record them in:
  - this tracker,
  - the opt-in tests’ skip messages / docs,
  - any future “golden quality” harness docs.

### 2) Introduce a **golden quality** evaluation lane

Deliverables (design + tooling + docs):

- A repeatable comparison workflow that evaluates `examples/result/<name>` against `examples/golden_result/<name>`:
  - Markdown normalization rules (line endings, trailing spaces, deterministic placeholder handling).
  - Image asset checks (existence + optional hash equality for `imgs/*`).
  - A gap report that is easy to read in PRs (summary + top diffs).

- An opt-in `swift test` suite (or a script-driven harness) that can:
  - **report** quality deltas without failing initially,
  - then progressively enforce **thresholds** per example as we converge.

Implementation details and phasing live in:
- `docs/dev_plans/quality_parity/implementation_plan.md`

### 3) Expand end-to-end parity coverage (upstream lane)

- Add opt-in parity tests for image examples in `examples/source/*.png` against `examples/reference_result/*` (Markdown + JSON).
- Keep tolerances and intentional diffs documented (link to `examples/reference_result_notes/*`).

### 4) Make the “match” definition explicit per artifact

Define “match” per output type:

- Markdown: exact match after normalization, or strict block-level match (headings/code blocks) with controlled fuzzing.
- JSON: schema + block count + bbox tolerance (already partially implemented).
- Images: file presence + size/hash policy.

### 5) Operationalize the workflow

- Document the recommended commands for:
  - running opt-in examples tests (env vars + `swift test --filter …`),
  - regenerating `examples/result/*` (`scripts/run_examples.sh`),
  - diffing against reference vs golden (what to compare first).

---

## Exit criteria

- A documented baseline snapshot + a repeatable parity workflow (**reference_result** lane).
- A documented baseline snapshot + a repeatable quality workflow (**golden_result** lane).
- The opt-in examples suite covers both PDFs and representative image cases.
- Remaining gaps are explicit and tracked per example (no “unknown parity/quality” phrasing once the table is in place).
